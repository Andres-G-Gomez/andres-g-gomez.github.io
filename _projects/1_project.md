---
layout: page
title: Affect-Aware Learning System
description: 
img: assets/img/asl_woman_L.jpg
importance: 1
category: Academic
related_publications: 
---

This project includes an assessment tool that discerns the correlation between a user's affective mannerisms and their performance, allowing for targeted interventions that enhance the learning experience. These interventions encompass recommending material review, breaks, or providing emotional support. The tool's versatility extends to various subjects, with our specific focus on learning American Sign Language (ASL). Using MediaPipe, features were extracted from webcam footage and fed to multiple modules, each designed for a particular task: head pose estimation, facial expression and ASL recognition. The predictions and other features were then fused to inform our rule-based selection of an optimal educational intervention via the action recommendation module.

<a href="{{prepend: 'assets/pdf/Adaptive_multimodal_learning_system.pdf' | relative_url}}">Read the full paper here</a>


<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/system.JPG" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Overview of the adaptive multimodal learning system architecture.
</div>

During the learner's interaction with educational videos or content, our system utilizes MediaPipe to extract fifty-two localized facial expressions and thirteen pose features. These localized facial expressions are quantified by their presence in a given frame, while pose attributes include x and y coordinates, along with a presence value. All features are normalized and constrained within the [0,1] range. In total, our system's facial expression recognition and head pose estimation modules yield 91 normalized affect features.

Upon completing the video or content review, the learner transitions to the assessment phase of the system. In this scenario, the learner is prompted to provide a sign corresponding to the letter under evaluation. Leveraging MediaPipe, hand landmarks from the submitted image are precisely identified, serving as reference points to crop an isolated sign with dimensions 64x64. This isolated sign is then input into the ASL recognition module, which features a convolutional neural network (CNN) inspired by a highperforming Kaggle submission [1]. This Kaggle submission was developed for an American Sign Language (ASL) dataset comprising 90,000 isolated ASL images spanning 29 classes [2]. 

<div class="row justify-content-sm-center">
    <div class="col-sm-8 mt-3 mt-md-0">
        {% include figure.html path="assets/img/fig1.JPG" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
    <div class="col-sm-4 mt-3 mt-md-0">
        {% include figure.html path="assets/img/fig2.JPG" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>

<div class="caption">
    The left figure shows the facial and pose landmarks extracted by MediaPipe. The dashed red boundary on the right include the thirteen extraced pose features. The right figure depicts the CNN architecture used within the ASL recognition module.
</div>

A probability of the user's isolated ASL submission is generated by the ASL recognition module. This information guides the action recommendation module. Currently, the action recommendation module operates on a rule-based system, relying solely on the probability of the true label generated by the ASL recognition module. If the score for the evaluated sign exceeds 80% accuracy, the module advises to 'continue to the next module'; for scores between 50% and 80%, the recommendation is to 'take a break'; and if it falls below 50%, the module suggests to 'review the module.' As this work progresses, we will explore more sophisticated methods for optimal intervention selection. This may involve refining the rules or incorporating reinforcement learning to develop a policy that maximizes user performance. 

{% raw %}

{% endraw %}
