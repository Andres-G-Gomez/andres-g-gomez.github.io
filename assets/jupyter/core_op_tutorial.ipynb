{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InternImage DCNv3 Code Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.init import xavier_uniform_, constant_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes an input tensor x and returns a modified version of it. In this case, it uses the permute method to change the order of dimensions in the tensor. The dimensions are reordered from the default order (batch_size, height, width, channels) to (batch_size, channels, height, width). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class to_channels_first(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, this function uses the permute method to change the order of dimensions in the tensor. The dimensions are reordered from the default order (batch_size, channels, height, width) to (batch_size, height, width, channels). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class to_channels_last(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.permute(0, 2, 3, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `build_norm_layer` function builds a normalization layer based on the type of normalization (`'BN'` for Batch Normalization or `'LN'` for Layer Normalization), and the input and output data formats (`'channels_first'` or `'channels_last'`). It dynamically creates a sequence of PyTorch layers, including format conversion layers (`to_channels_first` and `to_channels_last`) when needed, and returns a PyTorch `Sequential` container encapsulating the constructed normalization layers in the desired order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_norm_layer(dim,\n",
    "                     norm_layer,\n",
    "                     in_format='channels_last',\n",
    "                     out_format='channels_last',\n",
    "                     eps=1e-6):\n",
    "    layers = []\n",
    "    if norm_layer == 'BN':\n",
    "        if in_format == 'channels_last':\n",
    "            layers.append(to_channels_first())\n",
    "        layers.append(nn.BatchNorm2d(dim))\n",
    "        if out_format == 'channels_last':\n",
    "            layers.append(to_channels_last())\n",
    "    elif norm_layer == 'LN':\n",
    "        if in_format == 'channels_first':\n",
    "            layers.append(to_channels_last())\n",
    "        layers.append(nn.LayerNorm(dim, eps=eps))\n",
    "        if out_format == 'channels_first':\n",
    "            layers.append(to_channels_first())\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f'build_norm_layer does not support {norm_layer}')\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `build_act_layer` function constructs an activation layer based on the specified activation function type (`act_layer`). It supports Rectified Linear Unit (ReLU), Sigmoid Linear Unit (SiLU), and Gaussian Error Linear Unit (GELU) activation functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_act_layer(act_layer):\n",
    "    if act_layer == 'ReLU':\n",
    "        return nn.ReLU(inplace=True)\n",
    "    elif act_layer == 'SiLU':\n",
    "        return nn.SiLU(inplace=True)\n",
    "    elif act_layer == 'GELU':\n",
    "        return nn.GELU()\n",
    "\n",
    "    raise NotImplementedError(f'build_act_layer does not support {act_layer}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `_is_power_of_2` function checks if a given integer `n` is a power of 2. It first validates that `n` is a non-negative integer; otherwise, it raises a `ValueError`. The function then returns `True` if `n` is a power of 2 and not equal to 0. If the conditions are not met, it returns `False`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_power_of_2(n):\n",
    "    if (not isinstance(n, int)) or (n < 0):\n",
    "        raise ValueError(\n",
    "            \"invalid input for _is_power_of_2: {} (type: {})\".format(n, type(n)))\n",
    "\n",
    "    return (n & (n - 1) == 0) and n != 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `CenterFeatureScaleModule` class computes the center feature scale based on a linear projection of the input `query`. The linear projection is performed using `torch.nn.functional.linear`, applying a weight matrix (`center_feature_scale_proj_weight`) and a bias vector (`center_feature_scale_proj_bias`). The result is then passed through a sigmoid activation function using `.sigmoid()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenterFeatureScaleModule(nn.Module):\n",
    "    def forward(self,\n",
    "                query,\n",
    "                center_feature_scale_proj_weight,\n",
    "                center_feature_scale_proj_bias):\n",
    "        center_feature_scale = torch.nn.functional.linear(query,\n",
    "                                        weight=center_feature_scale_proj_weight,\n",
    "                                        bias=center_feature_scale_proj_bias).sigmoid()\n",
    "        return center_feature_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DCNv3_pytorch` class initializes a deformable convolution module with configurable parameters such as the number of channels (`channels`), kernel size (`kernel_size`), stride, padding, dilation, and group size. It includes depthwise convolution, offset and mask linear projections, and input/output linear projections. The module supports center feature scaling, where the scale is computed based on a learned projection. The parameters are initialized, and the forward method performs the actual deformable convolution operation. It handles the deformable convolution core (`dcnv3_core_pytorch`), applies depthwise convolution, computes offsets and masks, and incorporates center feature scaling if enabled. The output is linearly projected and returned. The class provides flexibility for deformable convolution with various configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `forward` method of the `DCNv3_pytorch` class processes input data through the deformable convolution module. The input is assumed to have dimensions (N, H, W, C), representing batch size, height, width, and channels, respectively. The method begins by applying a linear projection to the input (`input_proj`), and the result is stored in variable `x`. Another reference to this projection is kept in `x_proj`. The input is then permuted to have dimensions (N, C, H, W) for compatibility with the depthwise convolution (`dw_conv`) operation. The depthwise convolution is applied to obtain features (`x1`). The offset and mask are computed based on `x1`, and the mask is softmax-normalized along the channel dimension. The core deformable convolution operation (`dcnv3_core_pytorch`) is then applied using the computed offset and mask, along with other specified parameters. If center feature scaling is enabled, it computes the scale using the `center_feature_scale_module` and adjusts the output accordingly. Finally, the output is linearly projected using the `output_proj` layer, and the result is returned. The method efficiently performs deformable convolution with additional features like depthwise convolution and center feature scaling, providing a powerful and flexible operation for neural network architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCNv3_pytorch(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            channels=64,\n",
    "            kernel_size=3,\n",
    "            dw_kernel_size=None,\n",
    "            stride=1,\n",
    "            pad=1,\n",
    "            dilation=1,\n",
    "            group=4,\n",
    "            offset_scale=1.0, \n",
    "            act_layer='GELU',\n",
    "            norm_layer='LN',\n",
    "            center_feature_scale=False):\n",
    "        \"\"\"\n",
    "        DCNv3 Module\n",
    "        :param channels\n",
    "        :param kernel_size\n",
    "        :param stride\n",
    "        :param pad\n",
    "        :param dilation\n",
    "        :param group\n",
    "        :param offset_scale\n",
    "        :param act_layer\n",
    "        :param norm_layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if channels % group != 0:\n",
    "            raise ValueError(\n",
    "                f'channels must be divisible by group, but got {channels} and {group}')\n",
    "        _d_per_group = channels // group\n",
    "        dw_kernel_size = dw_kernel_size if dw_kernel_size is not None else kernel_size\n",
    "        # you'd better set _d_per_group to a power of 2 which is more efficient in our CUDA implementation\n",
    "        if not _is_power_of_2(_d_per_group):\n",
    "            warnings.warn(\n",
    "            \"You'd better set channels in DCNv3 to make the dimension of each attention head a power of 2 \"\n",
    "            \"which is more efficient in our CUDA implementation.\")\n",
    "\n",
    "        self.offset_scale = offset_scale\n",
    "        self.channels = channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dw_kernel_size = dw_kernel_size\n",
    "        self.stride = stride\n",
    "        self.dilation = dilation\n",
    "        self.pad = pad\n",
    "        self.group = group\n",
    "        self.group_channels = channels // group\n",
    "        self.center_feature_scale = center_feature_scale\n",
    "\n",
    "        self.dw_conv = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                channels,\n",
    "                channels,\n",
    "                kernel_size=dw_kernel_size,\n",
    "                stride=1,\n",
    "                padding=(dw_kernel_size - 1) // 2,\n",
    "                groups=channels),\n",
    "            build_norm_layer(\n",
    "                channels,\n",
    "                norm_layer,\n",
    "                'channels_first',\n",
    "                'channels_last'),\n",
    "            build_act_layer(act_layer))\n",
    "        \n",
    "        self.offset = nn.Linear( \n",
    "            channels,\n",
    "            group * kernel_size * kernel_size * 2)\n",
    "        \n",
    "        self.mask = nn.Linear(\n",
    "            channels,\n",
    "            group * kernel_size * kernel_size)\n",
    "        \n",
    "        self.input_proj = nn.Linear(channels, channels)\n",
    "        self.output_proj = nn.Linear(channels, channels)\n",
    "        self._reset_parameters()\n",
    "\n",
    "        # creates a trainable parameter center_feature_scale_proj_weight with shape \n",
    "        # (group, channels) and parameter center_feature_scale_proj_bias with shape \n",
    "        # (group, ) initialized with zeros.\n",
    "        if center_feature_scale:\n",
    "            self.center_feature_scale_proj_weight = nn.Parameter(\n",
    "                torch.zeros((group, channels), dtype=torch.float))\n",
    "            self.center_feature_scale_proj_bias = nn.Parameter(\n",
    "                torch.tensor(0.0, dtype=torch.float).view((1,)).repeat(group, ))\n",
    "            self.center_feature_scale_module = CenterFeatureScaleModule()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        constant_(self.offset.weight.data, 0.)\n",
    "        constant_(self.offset.bias.data, 0.)\n",
    "        constant_(self.mask.weight.data, 0.)\n",
    "        constant_(self.mask.bias.data, 0.)\n",
    "        xavier_uniform_(self.input_proj.weight.data)\n",
    "        constant_(self.input_proj.bias.data, 0.)\n",
    "        xavier_uniform_(self.output_proj.weight.data)\n",
    "        constant_(self.output_proj.bias.data, 0.)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        :param query                       (N, H, W, C)\n",
    "        :return output                     (N, H, W, C)\n",
    "        \"\"\"\n",
    "        N, H, W, _ = input.shape\n",
    "\n",
    "        # Linear projection of the input feature map\n",
    "        x = self.input_proj(input)\n",
    "        x_proj = x # Preserve a reference for later use\n",
    "\n",
    "        # Permute input dimensions for depthwise convolution\n",
    "        x1 = input.permute(0, 3, 1, 2) # (N, C, H, W)\n",
    "        x1 = self.dw_conv(x1) # Apply depthwise convolution\n",
    "        offset = self.offset(x1) # Compute offsets for deformable convolution\n",
    "\n",
    "        # self.mask(x1) outputs (N, H, W, self.group * kernel_size * kernel_size)\n",
    "        # The output is reshaped to have dimensions (N, H, W, self.group, -1). \n",
    "        # This reshaping is performed to create a set of masks for each group and position \n",
    "        # in the output feature map.\n",
    "        mask = self.mask(x1).reshape(N, H, W, self.group, -1) \n",
    "        # Applies the softmax function along the last dimension\n",
    "        mask = F.softmax(mask, -1).reshape(N, H, W, -1)\n",
    "\n",
    "        # Perform deformable convolution using the core operation\n",
    "        x = dcnv3_core_pytorch(\n",
    "            x, offset, mask,\n",
    "            self.kernel_size, self.kernel_size,\n",
    "            self.stride, self.stride,\n",
    "            self.pad, self.pad,\n",
    "            self.dilation, self.dilation,\n",
    "            self.group, self.group_channels,\n",
    "            self.offset_scale)\n",
    "\n",
    "        # Optionally, apply center feature scaling\n",
    "        if self.center_feature_scale:\n",
    "            center_feature_scale = self.center_feature_scale_module(\n",
    "                # linearly projects x1\n",
    "                x1, self.center_feature_scale_proj_weight, self.center_feature_scale_proj_bias)\n",
    "            \n",
    "            # reshapes and repeats 'center_feature_scale' to be compatible with 'x'\n",
    "            # N, H, W, groups -> N, H, W, groups, 1 -> \n",
    "            # N, H, W, groups, _d_per_group -> N, H, W, channels\n",
    "            center_feature_scale = center_feature_scale[..., None].repeat(\n",
    "                1, 1, 1, 1, self.channels // self.group).flatten(-2)\n",
    "\n",
    "            # Apply center feature scaling to the output\n",
    "            x = x * (1 - center_feature_scale) + x_proj * center_feature_scale\n",
    "            #  modulate the importance of features at different spatial locations \n",
    "        \n",
    "        # Linear projection of the output\n",
    "        x = self.output_proj(x)\n",
    "      \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `_get_reference_points` function computes reference points for deformable convolution given spatial shapes, kernel size, dilation, padding, and stride parameters. The spatial shapes are expected to be a tuple (N, H, W, C) representing batch size, height, width, and channels. The function first extracts the height and width values from the spatial shapes. It then calculates the output height and width based on the specified kernel size, dilation, padding, and stride.\n",
    "\n",
    "Next, it creates reference points (`ref_y` and `ref_x`) using `torch.meshgrid`. These points are generated based on the specified dilation, kernel size, and spatial dimensions, ensuring they cover the valid range for the deformable convolution operation. The generated points are reshaped and normalized to the spatial dimensions.\n",
    "\n",
    "Finally, the reference points are stacked along the last dimension and reshaped to have dimens (1, H_out, W_out, 1, 2) wherehere H_out and W_out are the calculated output height and width. The result represents the reference points used in deformable convolution and is returned by the function. This function is crucial for obtaining the spatial locations to sample in the input feature map during the deformable convolution operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_reference_points(spatial_shapes, device, kernel_h, kernel_w, dilation_h, dilation_w, \n",
    "                          pad_h=0, pad_w=0, stride_h=1, stride_w=1):\n",
    "    _, H_, W_, _ = spatial_shapes\n",
    "\n",
    "    # Calculate the output dimensions after convolution\n",
    "    H_out = (H_ - (dilation_h * (kernel_h - 1) + 1)) // stride_h + 1\n",
    "    W_out = (W_ - (dilation_w * (kernel_w - 1) + 1)) // stride_w + 1\n",
    "\n",
    "    # Generate grid of reference points in the input space\n",
    "    # ref_x and ref_y represent ~ HxW\n",
    "    ref_y, ref_x = torch.meshgrid(\n",
    "        torch.linspace(\n",
    "            # pad_h + 0.5,\n",
    "            # H_ - pad_h - 0.5,\n",
    "            (dilation_h * (kernel_h - 1)) // 2 + 0.5,\n",
    "            (dilation_h * (kernel_h - 1)) // 2 + 0.5 + (H_out - 1) * stride_h,\n",
    "            H_out,\n",
    "            dtype=torch.float32,\n",
    "            device=device),\n",
    "        torch.linspace(\n",
    "            # pad_w + 0.5,\n",
    "            # W_ - pad_w - 0.5,\n",
    "            (dilation_w * (kernel_w - 1)) // 2 + 0.5,\n",
    "            (dilation_w * (kernel_w - 1)) // 2 + 0.5 + (W_out - 1) * stride_w,\n",
    "            W_out,\n",
    "            dtype=torch.float32,\n",
    "            device=device))\n",
    "\n",
    "    # Flattens and normalizes the reference points to the range [0, 1]\n",
    "    ref_y = ref_y.reshape(-1)[None] / H_ \n",
    "    ref_x = ref_x.reshape(-1)[None] / W_\n",
    "\n",
    "    # Stack the reference points and reshape for compatibility with deformable convolution\n",
    "    ref = torch.stack((ref_x, ref_y), -1).reshape(\n",
    "        1, H_out, W_out, 1, 2)\n",
    "\n",
    "    return ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `_generate_dilation_grids` function is responsible for creating dilation grids used in the deformable convolution operation. It takes spatial shapes, kernel size, dilation factors, group size, and the device as input parameters. The spatial shapes are expected to be a tuple (N, H, W, C) representing batch size, height, width, and channels.\n",
    "\n",
    "The function initializes an empty list called `points_list` and then creates two sets of points along the x and y axes using `torch.meshgrid`. These points are generated based on the specified dilation, kernel size, and spatial dimensions.\n",
    "\n",
    "The points are normalized to the spatial dimensions and stored in the `points_list`. The function then stacks these points along the last dimension, reshapes the resulting tensor, and repeats it according to the specified group size. The final shape of the grid tensor is (1, 1, 1, group * kernel_h * kernel_w, 2).\n",
    "\n",
    "The generated grid is returned by the function. This grid is a key component for deformable convolution, as it defines the locations in the input feature map that will be sampled during the convolution operation based on the computed offsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_dilation_grids(spatial_shapes, kernel_h, kernel_w, dilation_h, dilation_w, group, device):\n",
    "    _, H_, W_, _ = spatial_shapes\n",
    "    points_list = []\n",
    "\n",
    "    # Generate a meshgrid of coordinates based on kernel size and dilation\n",
    "    x, y = torch.meshgrid(\n",
    "        torch.linspace(\n",
    "            -((dilation_w * (kernel_w - 1)) // 2),\n",
    "            -((dilation_w * (kernel_w - 1)) // 2) +\n",
    "            (kernel_w - 1) * dilation_w, kernel_w,\n",
    "            dtype=torch.float32,\n",
    "            device=device),\n",
    "        torch.linspace(\n",
    "            -((dilation_h * (kernel_h - 1)) // 2),\n",
    "            -((dilation_h * (kernel_h - 1)) // 2) +\n",
    "            (kernel_h - 1) * dilation_h, kernel_h,\n",
    "            dtype=torch.float32,\n",
    "            device=device))\n",
    "\n",
    "    # Normalize the coordinates to the range [-1, 1]\n",
    "    points_list.extend([x / W_, y / H_])\n",
    "\n",
    "    # Stack the normalized coordinates and reshape for deformable convolution compatibility\n",
    "    grid = torch.stack(points_list, -1).reshape(-1, 1, 2).\\\n",
    "        repeat(1, group, 1).permute(1, 0, 2)\n",
    "    grid = grid.reshape(1, 1, 1, group * kernel_h * kernel_w, 2)\n",
    "\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, this function performs deformable convolution on the input feature map using the provided offset, mask, and other parameters. It involves padding the input, computing reference points and dilation grids, and performing grid sampling for the deformable convolution. The result is a feature map with adjusted spatial locations based on the learned offset and mask.\n",
    "\n",
    "`input` is the linear projection of the input of shape (C, C), `offset` is the of shape (C, group * kernel_size * kernel_size * 2), `mask` is of shape (C, group * 3kernel_size * kernel_size). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcnv3_core_pytorch(\n",
    "        input, offset, mask, kernel_h,\n",
    "        kernel_w, stride_h, stride_w, pad_h,\n",
    "        pad_w, dilation_h, dilation_w, group,\n",
    "        group_channels, offset_scale):\n",
    "    \n",
    "    # Pad input feature map\n",
    "    input = F.pad(\n",
    "        input,\n",
    "        [0, 0, pad_h, pad_h, pad_w, pad_w])\n",
    "\n",
    "    # Extract input dimensions\n",
    "    N_, H_in, W_in, _ = input.shape\n",
    "    _, H_out, W_out, _ = offset.shape\n",
    "\n",
    "    # Compute reference points and dilation grids\n",
    "    ref = _get_reference_points(\n",
    "        input.shape, input.device, kernel_h, kernel_w, dilation_h, dilation_w, pad_h, pad_w, \n",
    "        stride_h, stride_w) # (1, H_out, W_out, 1, 2)\n",
    "    \n",
    "    grid = _generate_dilation_grids(\n",
    "        input.shape, kernel_h, kernel_w, dilation_h, dilation_w, \n",
    "        group, input.device) # (1, 1, 1, group * kernel_h * kernel_w, 2).\n",
    "    \n",
    "    # Compute spatial normalization factors\n",
    "    # (1, 1, 1, group * kernel_h * kernel_w * 2) \n",
    "    spatial_norm = torch.tensor([W_in, H_in]).reshape(1, 1, 1, 2).\\\n",
    "        repeat(1, 1, 1, group*kernel_h*kernel_w).to(input.device)\n",
    "    \n",
    "    # Compute sampling locations\n",
    "    # (N, H_out, W_out, group * kernel_h * kernel_w * 2)\n",
    "    sampling_locations = (ref + grid * offset_scale).repeat(N_, 1, 1, 1, 1).flatten(3, 4) \\\n",
    "    + offset * offset_scale / spatial_norm\n",
    "    \n",
    "    # Calculate constants\n",
    "    P_ = kernel_h * kernel_w\n",
    "    sampling_grids = 2 * sampling_locations - 1\n",
    "\n",
    "    # Reshape input \n",
    "    # N_, H_in, W_in, group*group_channels -> N_, H_in*W_in, group*group_channels -> \n",
    "    # N_, group*group_channels, H_in*W_in -> N_*group, group_channels, H_in, W_in\n",
    "    input_ = input.view(N_, H_in*W_in, group*group_channels).transpose(1, 2).\\\n",
    "        reshape(N_*group, group_channels, H_in, W_in)\n",
    "    # (group*N, group_channels, H_out + padding, W_out + padding)\n",
    "    \n",
    "    # Reshape sampling grid \n",
    "    # N_, H_out, W_out, group*P_*2 -> N_, H_out*W_out, group, P_, 2 -> \n",
    "    # N_, group, H_out*W_out, P_, 2 -> N_*group, H_out*W_out, P_, 2\n",
    "    sampling_grid_ = sampling_grids.view(N_, H_out*W_out, group, P_, 2).transpose(1, 2).\\\n",
    "        flatten(0, 1)\n",
    "    # (group*N, H_out * W_out, kernel_size * kernel_size, 2)\n",
    "\n",
    "    # Reshape sampling_grid, perform bilinear interpretation if points are not available\n",
    "    # N_*group, group_channels, H_out*W_out, P_\n",
    "    sampling_input_ = F.grid_sample(\n",
    "        input_, sampling_grid_, mode='bilinear', padding_mode='zeros', \n",
    "        align_corners=False) # (group*N, group_channels, H_out * W_out, kernel_size * kernel_size)\n",
    "    \n",
    "    # Reshape mask\n",
    "    # (N_, H_out, W_out, group*P_) -> N_, H_out*W_out, group, P_ -> \n",
    "    # (N_, group, H_out*W_out, P_) -> (N_*group, 1, H_out*W_out, P_)\n",
    "    mask = mask.view(N_, H_out*W_out, group, P_).transpose(1, 2).\\\n",
    "        reshape(N_*group, 1, H_out*W_out, P_)\n",
    "    # (group*N, 1, H_out * W_out, kernel_size * kernel_size)\n",
    "\n",
    "    # for all {i,j}, sum over groups: x_g(p_{i,j} + location-aware offsets) * m_{g,k}(i,j)\n",
    "    output = (sampling_input_ * mask).sum(-1).view(N_,\n",
    "                                                   group*group_channels, H_out*W_out) \n",
    "                                # (N, channels, H_out * W_out)\n",
    "    \n",
    "    # Transpose and reshape the output\n",
    "    return output.transpose(1, 2).reshape(N_, H_out, W_out, -1).contiguous() # (N, H_out, W_out, channels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using this module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([10, 256, 256, 64])\n",
      "Output shape: torch.Size([10, 256, 256, 64])\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of DCNv3_pytorch\n",
    "dcn_module = DCNv3_pytorch(channels=64, kernel_size=3)\n",
    "\n",
    "# Generate random input feature map\n",
    "batch_size = 10\n",
    "height = width = 256\n",
    "channels = 64\n",
    "random_input = torch.randn(batch_size, height, width, channels)\n",
    "\n",
    "# Forward pass through the DCNv3 module\n",
    "output = dcn_module(random_input)\n",
    "\n",
    "# Print the shapes of input and output\n",
    "print(\"Input shape:\", random_input.shape)\n",
    "print(\"Output shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
