<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Navigating the InternImage Model | Andres G. Gomez</title> <meta name="author" content="Andres G. Gomez"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/initials.JPG?c7eece51f82ad275a2b403a984e67c06"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://andres-g-gomez.github.io/projects/5_project/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Andres </span>G. Gomez</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/_pages/teaching.html"></a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Navigating the InternImage Model</h1> <p class="post-description"></p> </header> <article> <h1 class="bold-left-underlined">-A Deep Dive into its Encoder and Revolutionary DCNv3 Operator</h1> <p>By Andres G. Gomez</p> <p>I. Article Purpose</p> <p>I have written this article to dissect the intricacies of the InternImage model’s encoder and core operator, DCNv3. Upon delving into the model’s intricacies, I found that the paper lacked crucial details. Online resources explaining their method or the nuances of this model were also scarce. To bridge this gap, I delved into a thorough exploration of the code and engaged in numerous journal club presentations, along with discussions within my lab. Drawing from these experiences, I’ve compiled this explanation and tutorials, aiming to provide clarity and aid fellow enthusiasts in grasping the model’s complexities. While not intended as a standalone resource, this article’s purpose is to complement and bolster the explanations provided by the authors and their paper.</p> <p>II. InternImage</p> <p>The authors set out with a lofty ambition: to construct a unified model capable of accommodating various input data modules and combinations, and whose output can support any task and its combination [1]. The authors of InternImage argue that CNN-based models can rival or even outperform Vision Transformers (ViTs). Their work lays the foundation for a CNN-based model that seamlessly scales to accommodate large-scale parameters and data efficiently. Additionally, they introduce a groundbreaking operator, Deformable-Convolution-v3 (DCNv3), which adds a new dimension to the model’s capabilities.</p> <p>III. Encoder Architecture</p> <p>To provide a comprehensive understanding of the InternImage core operator, it’s beneficial to begin by examining the architecture of InternImage itself. As illustrated in Figure 1, we initiate the process by feeding an image of dimensions HxWx3 into the stem layer. This initial step yields a feature map characterized by C1 channels and reduced dimensions of H/4 by W/4. Subsequently, this feature map is propagated through multiple InternImage blocks, conveniently categorized into stages 1 through 4.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog_fig1.JPG-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog_fig1.JPG-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog_fig1.JPG-1400.webp"></source> <img src="/assets/img/blog_fig1.JPG" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 1: Overall architecture of the InternImage encoder. The core operator is DCNv3, and the basic block composes of layer normalization (LN) and feed-forward network (FFN), the stem and downsampling layers follows conventional CNN’s designs, where “s2” and “p1” mean stride 2 and padding 1, respectively. Constrained by the stacking rules, only 4 hyperparameters (C1, C0 , L1, L3) can decide a model variant [1]. The right figure depicts the depth of the basic block. </div> <p>In Figure 1, the deformable convolution operation is represented as DCNv3. Within each stage of the basic block, the input matrix undergoes convolution using this core operator. Subsequently, the process includes layer normalization, followed by summation with a skip connection, a feed-forward network, another layer normalization step, and finally summation with another skip connection. These operations can be iterated multiple times within a single stage, effectively increasing the number of parameters per stage. This iterative process enables the model to learn progressively intricate and abstract features.</p> <p>The right image in Figure 1 above illustrates the stacking and depth of these operations, which are regulated by parameter Li. The depths of these stages and the model sizes are determined by the guidance provided by Tan et al [2]. While the default depths for each stage are set at [4, 4, 18, 4], they may vary depending on the parameter size of the model.</p> <p>IV. DCNv3 core operator</p> <p>The basic idea behind the DCNv3 core operator is to introduce learnable offsets to the regular grid sampling locations used in standard convolutional operations. These offsets effectively enhance the flexibility of the convolution operations, allowing them to adaptively adjust their receptive field to better capture spatially variant features in the input data. Figure 2 illustrates the deformable convolution process, showcasing how the introduced offsets enable dynamic adjustments to the convolutional receptive field.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog_fig2.JPG-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog_fig2.JPG-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog_fig2.JPG-1400.webp"></source> <img src="/assets/img/blog_fig2.JPG" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 2: Deformable convolution principle, provided by Wang et al [3]. </div> <p>The overall architecture behind the DCNv3 core operator is illustrated in Figure 3. The input feature map x is inputted into the operator. Depthwise convolution, normalization, and activation are subsequently applied to x. This process is followed by linear layers tasked with learning offset parameters ∆pgk and modulation scalars (filter values) mgk. Here, g denotes the group to which the parameter belongs, and k corresponds to the k-th sampling point in the convolution filter. The utilization of groups facilitates the learning of distinct spatial aggregation patterns, thereby enhancing the robustness of features for downstream tasks.</p> <p>The input feature map x is then linearly projected, resulting in x’. Utilizing the learned sampling offsets, modulation scalars, and the projected input, we perform deformable convolution and layer normalization to obtain an output feature map y. Subsequently, we have the option to adaptively combine this output with the linearly projected input or proceed directly with y. The last step involves linearly projecting the output.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog_fig3.JPG-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog_fig3.JPG-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog_fig3.JPG-1400.webp"></source> <img src="/assets/img/blog_fig3.JPG" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 3: DCNv3 core operator. The input feature map and linearly projected input feature map are denoted by x and x’, respectively. The location irrelevant projection weights of each group are denoted by wg, mgk denotes the modulation scalar of the k-th sampling point, ∆pgk denotes the offsets, and yg denotes the output feature map of group g. </div> <p>The paper offers the following equation for DCNv3, where G represents the total number of aggregation groups. For the g-th group, wg ∈ RC×C’ denotes the location irrelevant projection weights of the group, where C’ =C/G represents the group dimension. mgk ∈ R denotes the modulation scalar of the k-th sampling point in the g-th group, normalized by the SoftMax function along the dimension K. xg ∈ RC’×H×W represents the sliced input feature map. ∆pgk is the offset corresponding to the grid sampling location pk in the g-th group [1].</p> <div class="row"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog_eq1.JPG-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog_eq1.JPG-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog_eq1.JPG-1400.webp"></source> <img src="/assets/img/blog_eq1.JPG" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>For clarity, we suggest some modifications to the above equation.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog_eq2.JPG-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog_eq2.JPG-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog_eq2.JPG-1400.webp"></source> <img src="/assets/img/blog_eq2.JPG" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>In this equation, it’s important to note that for each point (i, j), there exists a corresponding location-aware modulation scalar mgk and offset ∆pgk. Additionally, xg’ represents a linear projection of the input xg. This notation not only clarifies the mathematical operations but also facilitates understanding when referring to the corresponding code.</p> <p>V. Conclusion</p> <p>By addressing gaps in existing documentation and drawing from my own analysis, I’ve aimed to provide a thorough grasp of InternImage, enhanced with tutorials and explanations intended to assist both enthusiasts and practitioners. While this article serves to complement the work of the original authors and their paper, it also reflects the collaborative efforts within the research community to propel advancements in deep learning.</p> <p>For an in-depth explanation of the author’s DCNv3 code, please refer to my <a href="https://github.com/Andres-G-Gomez/andres-g-gomez.github.io/blob/master/assets/jupyter/core_op_tutorial.ipynb" rel="external nofollow noopener" target="_blank">tutorial</a>, which meticulously walks through nearly every line of code.</p> <p>VI. References</p> <p>[1] W. Wang et al., “Internimage: Exploring large-scale vision foundation models with deformable convolutions,” 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Jun. 2023. doi:10.1109/cvpr52729.2023.01385</p> <p>[2] M. Tan, and Q. V. Le, “EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,” International Conference on Machine Learning, 2019. doi:10.48550/arXiv.1905.11946</p> <p>[3] S. Wang, X. Xia, L. Ye, and B. Yang, “Automatic detection and classification of steel surface defect using deep convolutional neural networks,” Metals, vol. 11, no. 3, p. 388, Feb. 2021. doi:10.3390/met11030388</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Andres G. Gomez. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: April 05, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>