<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Chest X-Ray Classification | Andres G. Gomez</title> <meta name="author" content="Andres G. Gomez"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/initials.JPG?c7eece51f82ad275a2b403a984e67c06"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://andres-g-gomez.github.io/projects/8_project/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Andres </span>G. Gomez</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/_pages/teaching.html"></a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Chest X-Ray Classification</h1> <p class="post-description"></p> </header> <article> <h2 id="project-objective"><u>Project Objective</u></h2> <p>The goal is to design and train an image classification network to categorize X-ray images into one of four classes: COVID-19, Normal, Lung Opacity, and Viral Pneumonia.</p> <div class="container text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/8_project/imageClassification-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/8_project/imageClassification-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/8_project/imageClassification-1400.webp"></source> <img src="/assets/img/8_project/imageClassification.jpg" class="img-fluid d-block mx-auto w-50 w-md-75 w-lg-100" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <h2 id="dataset-overview"><u>Dataset Overview</u></h2> <p>The COVID-19 Chest X-Ray dataset is a comprehensive collection of X-ray images categorized into four distinct classes. This dataset is designed to aid in the development and evaluation of machine learning models for image classification and segmentation tasks related to lung diseases. Below is a detailed breakdown of the dataset:</p> <ul> <li> <strong>COVID-19 Positive:</strong> 3,616 images</li> <li> <strong>Normal:</strong> 10,192 images</li> <li> <strong>Lung Opacity (Non-COVID lung infection):</strong> 6,012 images</li> <li> <strong>Viral Pneumonia:</strong> 1,345 images</li> <li> <strong>Masks:</strong> Respective masks for each category</li> </ul> <div class="row justify-content-sm-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/8_project/lungs-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/8_project/lungs-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/8_project/lungs-1400.webp"></source> <img src="/assets/img/8_project/lungs.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Image samples from each class: A. Covid, B. Lung Opacity, C. Viral Pneumonia, D. Normal. </div> <h2 id="image-classification-models"><u>Image Classification Models</u></h2> <p>We evaluated several state-of-the-art image classification models, each with unique features and performance characteristics: VGG16, ResNet18, DenseNet121, ResNet152, and Vision Transformer (ViT). VGG16 is known for its simplicity and effectiveness with 16 layers. ResNet18 and ResNet152 use residual connections to mitigate the vanishing gradient problem, while DenseNet121 employs dense blocks for efficient feature reuse. ViT captures long-range dependencies using transformer architecture. This diverse exploration aimed to identify the most suitable model for accurate and robust Chest X-Ray image classification.</p> <div class="container text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/8_project/vgg16-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/8_project/vgg16-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/8_project/vgg16-1400.webp"></source> <img src="/assets/img/8_project/vgg16.jpg" class="img-fluid d-block mx-auto w-50 w-md-75 w-lg-100" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>I. <strong>VGG16</strong></p> <ul> <li>Introduced in 2014 by the University of Oxford.</li> <li>Achieved 92.7% top-5 accuracy on ImageNet.</li> <li>Consists of 16 layers: 13 convolutional and 3 fully connected.</li> <li>Total parameters: 138 million.</li> </ul> <div class="container text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/8_project/resnet18-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/8_project/resnet18-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/8_project/resnet18-1400.webp"></source> <img src="/assets/img/8_project/resnet18.jpg" class="img-fluid d-block mx-auto w-50 w-md-75 w-lg-100" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>II. <strong>ResNet18</strong></p> <ul> <li>Introduced in 2015 by Microsoft.</li> <li>Utilizes shortcut connections to mitigate the vanishing gradient problem.</li> <li>Comprises 72 layers with 11 million parameters.</li> </ul> <div class="container text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/8_project/densenet121-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/8_project/densenet121-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/8_project/densenet121-1400.webp"></source> <img src="/assets/img/8_project/densenet121.jpg" class="img-fluid d-block mx-auto w-50 w-md-75 w-lg-100" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>III. <strong>DenseNet121</strong></p> <ul> <li>Introduced in 2016 by Facebook AI Research.</li> <li>Features dense blocks with repeated convolution operations.</li> <li>Comprises 120 convolutions with 8 million parameters.</li> </ul> <div class="container text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/8_project/resnet152-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/8_project/resnet152-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/8_project/resnet152-1400.webp"></source> <img src="/assets/img/8_project/resnet152.jpg" class="img-fluid d-block mx-auto w-50 w-md-75 w-lg-100" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>IV. <strong>ResNet152</strong></p> <ul> <li>Introduced in 2015.</li> <li>Contains 152 layers with 60 million parameters.</li> <li>Known for its depth and high parameter count.</li> </ul> <div class="container text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/8_project/vit-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/8_project/vit-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/8_project/vit-1400.webp"></source> <img src="/assets/img/8_project/vit.jpg" class="img-fluid d-block mx-auto w-50 w-md-75 w-lg-100" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>V. <strong>Vision Transformer (ViT)</strong></p> <ul> <li>Introduced in 2020 by Google Research and Brain Team.</li> <li>Utilizes transformer architecture to capture long-range dependencies.</li> </ul> <h2 id="methods"><u>Methods</u></h2> <h3 id="data-augmentation-techniques">Data Augmentation Techniques</h3> <p>The paper “A Review of Medical Image Data Augmentation Techniques for Deep Learning Applications” (2021) discussed various augmentation methods, including</p> <ul> <li> <strong>Basic Augmentation:</strong> Geometric transforms, cropping, noise injection, etc.</li> <li> <strong>Deformable Augmentation:</strong> Spline interpolation, deformable image registration, etc.</li> <li> <strong>Deep Learning Augmentation:</strong> GAN-based methods and others.</li> </ul> <p>In this project, we conducted extensive experiments to evaluate the performance of various image classification models on the COVID-19 Chest X-Ray dataset. We tested different data augmentation techniques to improve model generalization and robustness. The experiments were designed to compare the performance with and without data augmentation, specifically focusing on:</p> <ul> <li>i. No Augmentation: Baseline for comparison</li> <li>ii. Left-right flip</li> <li>iii. Shrink and pad, left-right flip</li> </ul> <div class="row"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/8_project/lungsFlipped-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/8_project/lungsFlipped-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/8_project/lungsFlipped-1400.webp"></source> <img src="/assets/img/8_project/lungsFlipped.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Example Image 1" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/8_project/shrink-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/8_project/shrink-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/8_project/shrink-1400.webp"></source> <img src="/assets/img/8_project/shrink.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Example Image 2" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Data augmentation examples: The left image demonstrates a basic left-right flip, while the right image illustrates a combination of shrinking, zero padding, and a left-right flip. </div> <h3 id="training-setup">Training Setup</h3> <p>During the training phase, careful attention was paid to various hyperparameters and optimization techniques to ensure effective model training and convergence. The following configuration details were employed:</p> <ul> <li> <p><strong>Batch Size</strong>: The batch size refers to the number of training examples utilized in one iteration. A batch size of 32 was chosen to balance computational efficiency and model stability.</p> </li> <li> <p><strong>Criterion</strong>: The criterion, or loss function, is a measure of the model’s performance during training. In this project, both Cross Entropy Loss and its variants, Weighted Cross Entropy Loss and Balanced Cross Entropy Loss, were explored to address class imbalances in the dataset. These loss functions were chosen for their effectiveness in multi-class classification tasks.</p> </li> <li> <p><strong>Optimizer</strong>: Stochastic Gradient Descent (SGD) with momentum 0.9 was employed as the optimizer. SGD is a widely-used optimization algorithm for training deep neural networks. The momentum term helps accelerate SGD in relevant directions and dampens oscillations.</p> </li> <li> <p><strong>Learning Rate Scheduler</strong>: A step learning rate scheduler with 20 steps and a decay factor (gamma) of 0.9 was utilized to adjust the learning rate during training. This scheduler reduces the learning rate by a factor of gamma after a certain number of epochs, allowing for finer adjustments as training progresses.</p> </li> <li> <p><strong>Training Duration</strong>: The models were trained for a maximum of 400 epochs or until the validation loss showed signs of continuous increase. This early stopping mechanism helps prevent overfitting and ensures that the model is trained for an optimal number of epochs.</p> </li> </ul> <p>By carefully tuning these hyperparameters and employing optimization techniques, the training process aimed to strike a balance between model convergence, generalization, and computational efficiency.</p> <h2 id="results"><u>Results</u></h2> <h3 id="training-and-validation-accuracy">Training and Validation Accuracy</h3> <p>The experimental results demonstrate the efficacy of different data augmentation techniques and model architectures in classifying COVID-19 Chest X-Ray images. With no augmentation, DenseNet achieved a training accuracy of 95% and a validation accuracy of 90%. Left-right flipping augmentation further improved performance across all models, with VGG16 achieving a validation accuracy of 94% and ResNet18 achieving 86%.</p> <p><strong>Left-right Flip Augmentation:</strong></p> <table> <thead> <tr> <th>Model</th> <th>Train Acc (w/ Aug)</th> <th>Train Acc (w/o Aug)</th> <th>Val Acc (w/ Aug)</th> <th>Val Acc (w/o Aug)</th> </tr> </thead> <tbody> <tr> <td>DenseNet121</td> <td>1.00</td> <td>0.95</td> <td>0.94</td> <td>0.90</td> </tr> <tr> <td>VGG16</td> <td>1.00</td> <td>0.94</td> <td>0.94</td> <td>0.91</td> </tr> <tr> <td>ResNet18</td> <td>0.86</td> <td>0.85</td> <td>0.85</td> <td>0.84</td> </tr> <tr> <td>ViT</td> <td>0.97</td> <td>n/a</td> <td>0.91</td> <td>n/a</td> </tr> </tbody> </table> <p>Additionally, the combination of shrinking, zero padding, and left-right flipping demonstrated promising results, with DenseNet achieving a validation accuracy of 93%. These findings underscore the importance of data augmentation in enhancing model robustness and generalization. Further exploration of model architectures, such as Vision Transformer, holds potential for improving classification accuracy and addressing complex medical image analysis tasks.</p> <p><strong>Shrink and Pad, Left-right Flip Augmentation:</strong></p> <table> <thead> <tr> <th>Model</th> <th>Train Acc (w/ Aug)</th> <th>Train Acc (w/o Aug)</th> <th>Val Acc (w/ Aug)</th> <th>Val Acc (w/o Aug)</th> </tr> </thead> <tbody> <tr> <td>DenseNet121</td> <td>1.00</td> <td>0.95</td> <td>0.93</td> <td>0.90</td> </tr> <tr> <td>VGG16</td> <td>1.00</td> <td>0.94</td> <td>0.94</td> <td>0.91</td> </tr> <tr> <td>ResNet152</td> <td>0.84</td> <td>0.88</td> <td>0.85</td> <td>0.87</td> </tr> </tbody> </table> <h3 id="test-accuracy">Test Accuracy</h3> <p>The test accuracy results validate the models’ robustness on unseen data. With left-right flipping augmentation, DenseNet and VGG16 achieved test accuracies of 94% and 95%, respectively. The combination of shrinking, zero padding, and left-right flipping resulted in competitive performance, with DenseNet reaching a test accuracy of 93%. These findings affirm the models’ effectiveness in accurately classifying COVID-19 Chest X-Ray images, indicating their potential for practical use in clinical and research settings.</p> <p><strong>Left-right Flip Augmentation:</strong></p> <table> <thead> <tr> <th>Model</th> <th>Test Acc (w/ Aug)</th> <th>Test Acc (w/o Aug)</th> </tr> </thead> <tbody> <tr> <td>DenseNet121</td> <td>0.94</td> <td>0.94</td> </tr> <tr> <td>VGG16</td> <td>0.95</td> <td>0.94</td> </tr> <tr> <td>ResNet18</td> <td>0.86</td> <td>0.86</td> </tr> <tr> <td>ViT</td> <td>0.90</td> <td>n/a</td> </tr> </tbody> </table> <p><strong>Shrink and Pad, Left-right Flip Augmentation:</strong></p> <table> <thead> <tr> <th>Model</th> <th>Test Acc (w/ Aug)</th> <th>Test Acc (w/o Aug)</th> </tr> </thead> <tbody> <tr> <td>DenseNet121</td> <td>0.93</td> <td>0.94</td> </tr> <tr> <td>VGG16</td> <td>0.93</td> <td>0.94</td> </tr> <tr> <td>ResNet152</td> <td>0.84</td> <td>0.86</td> </tr> </tbody> </table> <h2 id="conclusion"><u>Conclusion</u></h2> <p>This project highlights the application of various data augmentation techniques and advanced image classification models to categorize COVID-19 chest X-ray images effectively. DenseNet121 and VGG16 models demonstrated high accuracy, particularly with data augmentation. The Vision Transformer also showed promising results, indicating the potential of transformer architectures in medical image classification tasks.</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Andres G. Gomez. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: May 29, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>